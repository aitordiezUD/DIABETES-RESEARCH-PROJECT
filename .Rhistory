plot(state.x77)
view(state.x77)
state.x77
plot(state.x77[,"Income"],state.x77[,"Illiteracy"], col = "red")
plot(state.x77[,"Income"],state.x77[,"Illiteracy"], col = 1:2
)
plot(state.x77[,"Income"],state.x77[,"Illiteracy"], col = c("red","blue"))
x <- runif(100)
plot(x, type="l")
plot(x, type="l",col="blue")
plot(1:8, sample(1:8), pch=24, cex=1.5, bg="orange",
main="Random points", xlab="x axis", ylab="y axis")
plot(1:8, sample(1:8), pch=24, cex=1, bg="orange",
main="Random points", xlab="x axis", ylab="y axis")
plot(1:8, sample(1:8), pch=24, cex=1.5, bg="orange",
main="Random points", xlab="x axis", ylab="y axis")
plot(1:8, sample(1:8), pch=24, cex=1, bg="orange",
main="Random points", xlab="x axis", ylab="y axis")
plot(1:8, sample(1:8), pch=24, cex=1, bg="orange",main="Random points", xlab="x axis", ylab="y axis")
plot(1:8, sample(1:8), pch=24, cex=1.5, bg="orange",main="Random points", xlab="x axis", ylab="y axis")
x <- rnorm(100)
y <- rnorm(100)
x
y
shiny::runApp('C:/Users/aitor/Downloads')
source("~/.active-rstudio-document")
install.packages("ggplot2")
source("~/.active-rstudio-document")
ggplot(datos, aes(x = Año, y = Valor, group = 1)) +
geom_line() +
labs(title = "Análisis Temporal", x = "Año", y = "Valor") +
theme_minimal()
source("~/.active-rstudio-document")
ggplot(datos, aes(x = Año, y = Valor, group = 1)) +
geom_line() +
labs(title = "Análisis Temporal", x = "Año", y = "Valor") +
theme_minimal()
?geom_histogram
pacman::p_load(
faraway
)
# Introduction
# we will use the faraway library
library(faraway)
# we will work with the Gala dataset
# Gala dataset: number of species of plants in the Galapago islands
data(gala)
# lets get a glimpse of the dataset
head(gala)
# the dataset covers 30 observations (islands) and 7 variables
dim(gala)
# therefore the independent variables are
x <- model.matrix(~Area+Elevation+Nearest+Scruz+Adjacent,data=gala)
View(x)
# and the dependent variable is
y <- gala$Species
# now we apply the formula for the estimation of beta
# t is the transpose
# %*% is the matrix multiplication
# and solve yields the inverse
solve(t(x)%*%x)%*%t(x)%*%y
# Model (library)
# let us now see how to use the Faraway library
# use the lm() command for linear models
mod1 <- lm(Species~Area+Elevation+Nearest+Scruz+Adjacent, data=gala)
# use residuals() for the residuals (difference between data and prediction)
residuals(mod1)
# use fitted() for the prediction of the model
fitted(mod1)
# use deviance() for the residual sum of squares
deviance(mod1)
# use df.residual() for the degrees of freedom
df.residual(mod1)
# remember this should be m-(n+1)
30-(5+1)
# and use coef() for the estimation of the parameters
coef(mod1)
# the model contains more information that we may extract
names(mod1)
# we may be interested in the command summary() for a summary of the results
summary(mod1)
# the summary provides access to even more information
mod1s <- summary(mod1)
names(mod1s)
# for instance we can obtain R^2 from the summary
mod1s$r.squared
# which can also be computed with the corresponding formula
1-deviance(mod1)/sum((y-mean(y))^2)
# Inference
# Variances and standard deviations of the errors and parameters given their models
# We can use the summary it to compute sigma (standard deviation of errors)
mod1s$sigma
# Inference
# Variances and standard deviations of the errors and parameters given their models
# We can use the summary it to compute sigma (standard deviation of errors)
mod1s$sigma
# which is also available via the residual sum of squares and the degrees of freedom
sqrt(deviance(mod1)/df.residual(mod1))
# we can also compute the standard deviation of the parameters with the summary
mod1s$coef[,2]
# which are also available with the corresponding formula
sqrt(diag(solve(t(x)%*%x)*mod1s$sigma^2))
# Confidence intervals
# Confidence interval for the parameters
# we can obtain the boundaries with confint for a value of alpha=0.05
confint(mod1)
install.packages("knitr")
iris2 <- head(iris)
knitr::kable(iris2, col.names = gsub("[.]", " ", names(iris)))
install.packages("kableExtra")
kable(iris2, format = "html", caption = "Mi dataframe") %>%
kable_styling(
font_size = 12,
background_color = "white",
border_color = "black"
)
library(tidyverse)
kable(iris2, format = "html", caption = "Mi dataframe") %>%
kable_styling(
font_size = 12,
background_color = "white",
border_color = "black"
)
library(kableExtra)
library(knitr)
kable(iris2, format = "html", caption = "Mi dataframe") %>%
kable_styling(
font_size = 12,
background_color = "white",
border_color = "black"
)
??read_html
??html_nodes
??rank
?rank
?values
?seq
??amatch
?amatch
??stringdist::amatch
install.packages("stringdist")
?stringdist::amatch
?read_csv
?read.csv()
??dummy_cols
?corrplot::corrplot.mixed()
?corrplot
?corrplot::corrplot.mixed()
?dist
library(kableExtra)
kbl(dt,"latex")
dt <- mtcars[1:5, 1:6]
library(kableExtra)
kbl(dt,"latex")
library(kableExtra)
kbl(dt)
kbl(dt, booktabs = T)
library(kableExtra)
kbl(dt,"latex")
kbl(dt, booktabs = T)
kbl(mtcars[1:8, 1:4], booktabs = T, linesep = "") %>%
kable_styling(latex_options = "striped", stripe_index = c(1,2, 5:6))
?read.csv
NA>1.4
?knitr::kable
iris2 <- head(iris)
knitr::kable(iris2, col.names = gsub("[.]", " ", names(iris)))
library(knitr)
library(kableExtra)
kable(head(iris, 5), align = 'c', booktabs = TRUE) %>%
row_spec(1, bold = TRUE, italic = TRUE) %>%
row_spec(2:3, color = 'white', background = 'black') %>%
row_spec(4, underline = TRUE, monospace = TRUE) %>%
row_spec(5, angle = 45) %>%
column_spec(5, strikeout = TRUE)
?column_spec
```{r}
```{r}
df2 <- data.frame(translations = c("ta nipfey tag mud ta cep mud ta did bedwat ulu Norris nola mud Frodo Baggins.  pik tos solo 5 fog lugol, sim ulu roundhouse-gnupea Sauron's butt halfway thru ta prima koimem.",
"ulu Norris tis pstgul ta ell ka nunu bapple, yikai. . ex coopee azo hit corro ta ta daa yeh ex's lib a upospa, Yi kai yai yai!  ulu Norris tis adtut lib a upospa.",
"een ta pica nos bodib ta ditmop..  pelo yob bodib nos?  - pik tos ulu Norris!",
"ulu Norris' jugbys tau gag...  1)porp-AU-purcam, dah mogcab 12, 2010 2)addon sun sun sukhep 04, 2010 3)RYUKYU cooumm, gapay sukhep 26, 2010 4)MAULE, lika sukhep 27, 2010 5)kabis SUMATRA, ourreb dug 05, 2010 6)vetli burkey dug 08, 2010 hmmmmmm.",
"temer ulu Norris merciless upsmun 27 Ninjas weebo vacationing een gapay, ta aibug obidan bap pak ex soko be dos.  lo yapapo tos, \"domo vox cama pik, ka bedy am\".",
"asa bada tos a spaghetti cama ulu Norris to'd be godcee da hopa pik feguin alga veela to'd mac pik aka pik polo roundhouse sau to een ta balls."))
# Imprimir usando kable con ancho máximo de columna de 100 caracteres
kable(df2, "simple")
library(dplyr)
kable(df2) %>% column_spec(1,width = "10cm")
kable(df2) %>% column_spec(1,width = "5cm")
?kable
library(formattable)
?palmerpenguins
??palmerpenguins
install.packages("formattable")
?formattable::formattable()
??str_extract
install.packages("stringr")
?str_extract
?stringr::str_extract
stringr::fruit
library(stringr)
str_extract(fruit, "[aeiou]")
str_subset(fruit, "b")
prueba = "The Glory: Season 1 // 더 글로리: 시즌 1"
splt = str_split(prueba,"//")
splt
splt2 = str_split(prueba,"//",simplify = TRUE)
splt2
splt[[1]]
class(splt)
splt[[1]][1]
?lubridate::wday
weekdays_lubridate <- weekdays()[1:7]
??createDataPartition
??createDataPartition
?replace
??knn
install.packages("caret")
install.packages("caret")
??knn
install.packages("FNN")
install.packages("FNN")
?FNN::knnReg
data = read.csv("https://raw.githubusercontent.com/datasets/covid-19/main/data/key-countries-pivoted.csv")
View(data)
dim(data)
data[816,]
new_df = data.frame(matrix(unlist(column), nrow=1, byrow=T))
column <- t(data[816,])
new_df = data.frame(matrix(unlist(column), nrow=1, byrow=T))
View(new_df)
column <- t(data[816,])
new_df = data.frame(matrix(unlist(column)))
new_df
new_df = data.frame(cases = matrix(unlist(column)))
new_df
new_df[-1,]
new_df = new_df[-1,]
mew_df
library("dplyr")
new_df
new_df = data.frame(cases = matrix(unlist(column)))
new_df
new_df = new_df[-1,]
new_df
new_df = data.frame(cases = matrix(unlist(column)))
new_df = new_df[2:9,]
new_df
data = read.csv("https://raw.githubusercontent.com/datasets/covid-19/main/data/key-countries-pivoted.csv")
column <- t(data[816,])
new_df = data.frame(cases = matrix(unlist(column)))
new_df = new_df %>% mutate(date = colnames(new_df))
new_df = new_df[2:9,]
new_df
data = read.csv("https://raw.githubusercontent.com/datasets/covid-19/main/data/key-countries-pivoted.csv")
column <- t(data[816,])
new_df = data.frame(cases = matrix(unlist(column)))
new_df = new_df %>% mutate(date = colnames(data))
new_df = new_df[2:9,]
new_df
data = read.csv("https://raw.githubusercontent.com/datasets/covid-19/main/data/key-countries-pivoted.csv")
column <- t(data[816,])
new_df = data.frame(country = colnames(data)
new_df = new_df[2:9,]
new_df
new_df = data.frame(country = colnames(data))
new_df = new_df %>% mutate(cases = matrix(unlist(column)))
new_df = new_df[2:9,]
new_df
write.csv(new_df, "C:\\Users\\aitor\\OneDrive\\Escritorio\\DATA VISUALIZATION\\DELIVERABLE 4")
write.csv(new_df, "C:\\Users\\aitor\\OneDrive\\Escritorio\\DATA VISUALIZATION\\DELIVERABLE 4\\total.csv")
rownames(new_df) = NULL
write.csv(new_df, "C:\\Users\\aitor\\OneDrive\\Escritorio\\DATA VISUALIZATION\\DELIVERABLE 4\\total.csv")
new_df
write.csv(new_df, "C:\\Users\\aitor\\OneDrive\\Escritorio\\DATA VISUALIZATION\\DELIVERABLE 4\\total.csv",row.names=FALSE)
View(new_df)
data = read.csv("https://raw.githubusercontent.com/datasets/covid-19/main/data/key-countries-pivoted.csv")
column <- t(data[816,])
new_df = data.frame(Country = colnames(data))
new_df = new_df %>% mutate(Cases = matrix(unlist(column)))
new_df = new_df[2:9,]
rownames(new_df) = NULL
View(new_df)
data = read.csv("https://raw.githubusercontent.com/datasets/covid-19/main/data/key-countries-pivoted.csv")
column <- t(data[816,])
new_df = data.frame(Country = colnames(data))
new_df = new_df %>% mutate(Cases = matrix(unlist(column)))
new_df = new_df %>% slice(2:9)
rownames(new_df) = NULL
View(column)
View(new_df)
newdf
new_df
class(new_df$Cases)
data = read.csv("https://raw.githubusercontent.com/datasets/covid-19/main/data/key-countries-pivoted.csv")
column <- t(data[816,])
new_df = data.frame(Country = as.character(colnames(data)))
new_df = new_df %>% mutate(Cases = as.numeric(matrix(unlist(column))))
new_df = new_df %>% slice(2:9)
rownames(new_df) = NULL
write.csv(new_df, "C:\\Users\\aitor\\OneDrive\\Escritorio\\DATA VISUALIZATION\\DELIVERABLE 4\\total.csv",row.names=FALSE)
View(new_df)
pacman::p_load(
tidyverse,
dplyr,
FactoMineR,
factoextra,
stats,
plotly,
gplots,
corrplot,
kableExtra
)
#####################################################################
#PCA (Code used for the Report interpretation)
pca3 <- PCA(d3 %>% select(-Outcome),scale.unit=TRUE)
#Read the dataset
d3 = read.csv("DESCRIPTIVE ANALYSIS/DATASET 3/diabetes_d3.csv")
#Filtering the dataset, eliminating outliers
d3 = d3 %>% filter(BloodPressure > 0,BMI > 0, Glucose > 0, Insulin > 0, SkinThickness > 0)
#Converting the Outcome variable to a factor:
d3 = d3 %>% mutate(Outcome = ifelse(Outcome == 1, "Diabetic", "Non-Diabetic"))
d3$Outcome = as.factor(d3$Outcome)
#####################################################################
#PCA (Code used for the Report interpretation)
pca3 <- PCA(d3 %>% select(-Outcome),scale.unit=TRUE)
#Read the dataset
d3 = read.csv("DESCRIPTIVE ANALYSIS/DATASET 3/diabetes_d3.csv")
setwd("C:/Users/aitor/OneDrive/Escritorio/ADVANCED STATISTICS/DIABETES RESEARCH PROJECT")
#Read the dataset
d3 = read.csv("DESCRIPTIVE ANALYSIS/DATASET 3/diabetes_d3.csv")
#Filtering the dataset, eliminating outliers
d3 = d3 %>% filter(BloodPressure > 0,BMI > 0, Glucose > 0, Insulin > 0, SkinThickness > 0)
#Converting the Outcome variable to a factor:
d3 = d3 %>% mutate(Outcome = ifelse(Outcome == 1, "Diabetic", "Non-Diabetic"))
d3$Outcome = as.factor(d3$Outcome)
#####################################################################
#PCA (Code used for the Report interpretation)
pca3 <- PCA(d3 %>% select(-Outcome),scale.unit=TRUE)
#####################################################################
#K-Means Clustering
# K-means #
q <- 3
km.out <- kmeans(pca3$ind$coord[,1:2], q, nstart = 20)
fviz_cluster(km.out, data = pca3$ind$coord[,1:2])
fviz_cluster(km.out, data = pca3$ind$coord[,1:2], geom = "point", ellipse = TRUE, ellipse.type = "convex",
ellipse.level = 0.95, main = paste("K-Means Clustering Results with K = ", q, sep = " "))
fviz_cluster(km.out, data = pca3$ind$coord[,1:2], geom = "point", ellipse = TRUE, ellipse.type = "convex",
ellipse.level = 0.95, main = paste("K-Means Clustering Results with K = ", q, sep = " ")) +
theme_minimal()
q <- 2
km.out2 <- kmeans(pca3$ind$coord[,1:2], q, nstart = 20)
fviz_cluster(km.out2, data = pca3$ind$coord[,1:2])
fviz_cluster(km.out2, data = pca3$ind$coord[,1:2], geom = "point", ellipse = TRUE, ellipse.type = "convex",
ellipse.level = 0.95, main = paste("K-Means Clustering Results with K = ", q, sep = " "))
fviz_cluster(km.out2, data = pca3$ind$coord[,1:2], geom = "point", ellipse = TRUE, ellipse.type = "convex",
ellipse.level = 0.95, main = paste("K-Means Clustering Results with K = ", q, sep = " ")) +
theme_minimal()
##################################################################
#Hierarchical Clustering
# Hierarchical clustering #
#We first have to compute   a Distance Matrix, where we can choose among different metrics, e.g., euclidean/manhattan:
distance_matrix <- dist(pca3$ind$coord[,1:2], method = "euclidean")
distance_matrix
#Then, we apply the hclust function choosing the method (e.g., complete/single)
hc <-  hclust(distance_matrix, method = "complete")
hc
#We can plot the dendrogram
plot(hc)
#Two clusters:
fviz_dend(hc, k = 3, rect = TRUE, cex = 0.5, show_labels = FALSE, k_colors = c("#470500", "#c4d417", "#1fa4cc"))
#Three clusters:
fviz_dend(hc, k = 2, rect = TRUE, cex = 0.5, show_labels = FALSE, k_colors = c("#470500", "#1fa4cc"))
#Two clusters:
fviz_dend(hc, k = 3, rect = TRUE, cex = 0.5, show_labels = FALSE, k_colors = c("#470500", "#c4d417", "#1fa4cc"))
?hclust
#Three clusters:
fviz_dend(hc, k = 2, rect = TRUE, cex = 0.5, show_labels = FALSE, k_colors = c("#470500", "#1fa4cc"))
#Then, we apply the hclust function choosing the method (e.g., complete/single)
hc <-  hclust(distance_matrix, method = "single")
hc
#We can plot the dendrogram
plot(hc)
#Two clusters:
fviz_dend(hc, k = 3, rect = TRUE, cex = 0.5, show_labels = FALSE, k_colors = c("#470500", "#c4d417", "#1fa4cc"))
?hclust
#Then, we apply the hclust function choosing the method (e.g., complete/single)
hc <-  hclust(distance_matrix, method = "average")
hc
#We can plot the dendrogram
plot(hc)
#Two clusters:
fviz_dend(hc, k = 3, rect = TRUE, cex = 0.5, show_labels = FALSE, k_colors = c("#470500", "#c4d417", "#1fa4cc"))
#Three clusters:
fviz_dend(hc, k = 2, rect = TRUE, cex = 0.5, show_labels = FALSE, k_colors = c("#470500", "#1fa4cc"))
?dist
##################################################################
#Hierarchical Clustering
# Hierarchical clustering #
#We first have to compute   a Distance Matrix, where we can choose among different metrics, e.g., euclidean/manhattan:
distance_matrix <- dist(pca3$ind$coord[,1:2], method = "manhattan")
distance_matrix
#Then, we apply the hclust function choosing the method (e.g., complete/single)
hc <-  hclust(distance_matrix, method = "average")
hc
#We can plot the dendrogram
plot(hc)
#Two clusters:
fviz_dend(hc, k = 3, rect = TRUE, cex = 0.5, show_labels = FALSE, k_colors = c("#470500", "#c4d417", "#1fa4cc"))
#Three clusters:
fviz_dend(hc, k = 2, rect = TRUE, cex = 0.5, show_labels = FALSE, k_colors = c("#470500", "#1fa4cc"))
#To determine the cluster levels for each observation associated with a given cut of the dendrogram, we can use the \texttt{cutree()} function :
cutree(hc, 3)
##################################################################
#Hierarchical Clustering
# Hierarchical clustering #
#We first have to compute   a Distance Matrix, where we can choose among different metrics, e.g., euclidean/manhattan:
distance_matrix <- dist(pca3$ind$coord[,1:2], method = "manhattan")
distance_matrix
#Then, we apply the hclust function choosing the method (e.g., complete/single)
hc <-  hclust(distance_matrix, method = "complete")
hc
#We can plot the dendrogram
plot(hc)
##################################################################
#Hierarchical Clustering
# Hierarchical clustering #
#We first have to compute   a Distance Matrix, where we can choose among different metrics, e.g., euclidean/manhattan:
distance_matrix <- dist(pca3$ind$coord[,1:2], method = "manhattan")
distance_matrix
#Then, we apply the hclust function choosing the method (e.g., complete/single)
hc <-  hclust(distance_matrix, method = "single")
hc
#We can plot the dendrogram
plot(hc)
#Two clusters:
fviz_dend(hc, k = 3, rect = TRUE, cex = 0.5, show_labels = FALSE, k_colors = c("#470500", "#c4d417", "#1fa4cc"))
#Three clusters:
fviz_dend(hc, k = 2, rect = TRUE, cex = 0.5, show_labels = FALSE, k_colors = c("#470500", "#1fa4cc"))
##################################################################
#Hierarchical Clustering
# Hierarchical clustering #
#We first have to compute   a Distance Matrix, where we can choose among different metrics, e.g., euclidean/manhattan:
distance_matrix <- dist(pca3$ind$coord[,1:2], method = "manhattan")
distance_matrix
##################################################################
#Hierarchical Clustering
# Hierarchical clustering #
#We first have to compute   a Distance Matrix, where we can choose among different metrics, e.g., euclidean/manhattan:
distance_matrix <- dist(pca3$ind$coord[,1:2], method = "complete")
##################################################################
#Hierarchical Clustering
# Hierarchical clustering #
#We first have to compute   a Distance Matrix, where we can choose among different metrics, e.g., euclidean/manhattan:
distance_matrix <- dist(pca3$ind$coord[,1:2], method = "euclidean")
distance_matrix
#Then, we apply the hclust function choosing the method (e.g., complete/single)
hc <-  hclust(distance_matrix, method = "complete")
hc
#We can plot the dendrogram
plot(hc)
##################################################################
#Hierarchical Clustering
# Hierarchical clustering #
#We first have to compute   a Distance Matrix, where we can choose among different metrics, e.g., euclidean/manhattan:
distance_matrix <- dist(pca3$ind$coord[,1:2], method = "euclidean")
distance_matrix
#Then, we apply the hclust function choosing the method (e.g., complete/single)
hc <-  hclust(distance_matrix, method = "complete")
hc
#We can plot the dendrogram
plot(hc)
#Two clusters:
fviz_dend(hc, k = 3, rect = TRUE, cex = 0.5, show_labels = FALSE, k_colors = c("#470500", "#c4d417", "#1fa4cc"))
#Three clusters:
fviz_dend(hc, k = 2, rect = TRUE, cex = 0.5, show_labels = FALSE, k_colors = c("#470500", "#1fa4cc"))
#####################################################################
#PCA (Code used for the Report interpretation)
pca3 <- PCA(d3 %>% select(-Outcome),scale.unit=TRUE)
#Scree plot
fviz_eig(pca3,addlabels = TRUE, ylim = c(0, 50),
barfill = "steelblue",barcolor = "black",
linecolor = "black",title ="% of explained variance by each component")
#Plot for variables
fviz_pca_var(pca3, repel = TRUE) +
theme_minimal()
#Plot for individuals
fviz_pca_ind(pca3, label="none", habillage=d3$Outcome,
addEllipses=TRUE, ellipse.level=0.95) +
scale_color_brewer(palette="Set1") +
theme_minimal()
#BIPLOT: Individuals and Variables
fviz_pca_biplot(pca3, label="var",col.var = "black", repel=TRUE, habillage=d3$Outcome) +
labs(title = "PCA - Individuals and Variables") +
scale_color_brewer(palette="Set1") +
theme_minimal()
#BIPLOT: Individuals and Variables with ellipses
fviz_pca_biplot(pca3, label="var",col.var = "black", repel=TRUE, habillage=d3$Outcome,
addEllipses=TRUE, ellipse.level=0.8) +
labs(title = "PCA - Individuals and Variables") +
scale_color_brewer(palette="Set1") +
theme_minimal()
#BIPLOT: Individuals and Variables
fviz_pca_biplot(pca3, label="var",col.var = "black", repel=TRUE, habillage=d3$Outcome) +
labs(title = "PCA - Individuals and Variables") +
scale_color_brewer(palette="Set1") +
theme_minimal()
#Plot for individuals
fviz_pca_ind(pca3, label="none", habillage=d3$Outcome,
addEllipses=TRUE, ellipse.level=0.95) +
scale_color_brewer(palette="Set1") +
theme_minimal()
